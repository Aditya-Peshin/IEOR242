install.packages("fields")
install.packages(c('chron','colorspace','codetools','DBI','devtools','dichromat','digest','doParallel', 'dplyr', 'fields',
'foreach', 'ggplot2','gridExtra','gtable','inline','iterators','knitr','labeling','lattice','lme4','mapproj','maps','munsell','proftools',
'proto','purrr', 'rbenchmark','RColorBrewer','Rcpp','reshape2','rJava','RSQLite','scales','spam',
'stringr','tidyr','xlsx','xlsxjars','xtable'), repos = "https://cran.cnr.berkeley.edu")
gap <- read.csv(file.path('..', 'data', 'gapminder-FiveYearData.csv'), stringsAsFactors = FALSE)
gap <- read.csv(file.path("F:\\r-bootcamp-fall-2019-master\\r-bootcamp-fall-2019-master\\data\\gapminder-FiveYearData.csv"), stringsAsFactors = FALSE)
gap2007 <- gap[gap$year == 2007, ]
ord <- order(gap$year, gap$lifeExp, decreasing = TRUE)
ord[1:5]
gm_ord <- gap[ord, ]
sortByCol <- function(data, col1, col2) {
# Sorts a matrix or dataframe based on two  columns
#
# Args:
#     data: a dataframe or matrix with at least columns
#                  and any number of rows
#     col1: a reference to the column to sort on
#     col2: a reference to the column to use for ties
#
# Returns:
#     <data> sorted in increasing order by the values
#     in the first column. Any ties should be broken by values
#     in the second column. The row pairs should be maintained
#     in this result
ord <- order(data[, col1], data[, col2], decreasing=TRUE)
sorted <- data[ord, ]
return(sorted)
}
identical(gm_ord, sortByCol(gap, "year", "lifeExp"))
sortByCol <- function(data, col1 = 1, col2 = 2) {
ord <- order(data[, col1], data[, col2], decreasing=TRUE)
sorted <- data[ord, ]
return(sorted)
}
identical(sortByCol(gap, 1, 2), sortByCol(gap))
identical(sortByCol(col2 = 2, data = gap, col1 = 1), sortByCol(gap, 1, 2))
pplot <- function(x, y, ...) {
plot(x, y, pch = 16, cex = 0.6, ...)
}
pplot(gap$gdpPercap, gap$lifeExp,  xlab = 'gdpPercap (log $)',
ylab = 'life expectancy (years)', log = 'x')
x <- 2
f <- function(y) {
return(x + y)
}
f(1)
g <- function(y) {
x <- 10
return(f(y))
}
g(1)
g <- function(y) {
f <- function(y) {
return(x + y)
}
x <- 10
return(f(y))
}
g(1)
3+2
'+'(3,2)
x[ , 2]
x <- matrix(runif(100), 10)
x[ , 2]
'['(x , , 2)
class(1)
class(runif)
class(function(x) x^2)
square <- function(x) x^2
class(square)
class(1)
class(runif)
class(function(x) x^2)
square <- function(x) x^2
class(square)
dim(x)
?matrix
function miniszero(x) {
x[x<0] = 0
}
function miniszero(x) {
x[(x<0)] <- 0
}
function miniszero(x) {
vals <- x < 0
}
?function
()
}
?function()
}
help("function")
miniszero <- function (x) {
vals <- x < 0
}
miniszero <- function (x) {
x[x < 0] = 0
}
abc = [ 1, 2, 3, -4, 5 ,-7,9]
abc
miniszero(abc)
abc
miniszero <- function (x) {
x[x < 0] = 0
return x
}
miniszero <- function (x) {
x[x < 0] = 0
return x
}
miniszero <- function (x) {
x[x < 0] = 0
return(x)
}
abc = [ 1, 2, 3, -4, 5 ,-7,9]
abc = c(1, 2, 3, -4, 5 ,-7,9)
abc
miniszero(abc)
abc
abc = miniszero(abc)
abc
# NOT run
install.packages('dplyr')
library(dplyr)
year_country_gdp_dplyr <- select(gap, year, country, gdpPercap)
head(year_country_gdp_dplyr)
year_country_gdp_base <- gap[,c("year", "country", "gdpPercap")]
head(year_country_gdp_base)
# checking equivalence: TRUE indicates an exact match between these objects
all.equal(year_country_gdp_dplyr, year_country_gdp_base)
year_country_gdp <- gap %>% select(year, country, gdpPercap)
dim(year_country_gdp)
year_country_gdp_africa <- gap %>%
filter(continent == "Africa") %>%
select(year,country,gdpPercap)
dim(year_country_gdp_africa)
gdp_bycontinents <- gap %>%
group_by(continent) %>%
summarize(mean_gdpPercap = mean(gdpPercap))
head(gdp_bycontinents)
gdp_pop_bycontinents_byyear <- gap %>%
group_by(continent, year) %>%
summarize(mean_gdpPercap = mean(gdpPercap),
sd_gdpPercap = sd(gdpPercap),
mean_pop = mean(pop),
sd_pop = sd(pop))
head(gdp_pop_bycontinents_byyear)
head(gdp_pop_bycontinents_byyear)
gap_with_extra_vars <- gap %>%
group_by(continent, year) %>%
mutate(mean_gdpPercap = mean(gdpPercap),
sd_gdpPercap = sd(gdpPercap),
mean_pop = mean(pop),
sd_pop = sd(pop)) %>%
arrange(desc(year), continent) # `desc()` puts things ins descending order
head(gap_with_extra_vars)
gdp_pop_bycontinents_byyear <- gap %>%
mutate(gdp_billion = gdpPercap*pop/10^9) %>%
group_by(continent, year) %>%
summarize(mean_gdpPercap = mean(gdpPercap),
sd_gdpPercap = sd(gdpPercap),
mean_pop = mean(pop),
sd_pop = sd(pop),
mean_gdp_billion = mean(gdp_billion),
sd_gdp_billion = sd(gdp_billion))
head(gdp_pop_bycontinents_byyear)
gap_with_extra_vars <- gap %>%
group_by(continent, year) %>%
mutate(mean_gdpPercap = mean(gdpPercap),
sd_gdpPercap = sd(gdpPercap),
mean_pop = mean(pop),
sd_pop = sd(pop)) %>%
arrange(desc(year), continent) # `desc()` puts things ins descending order
head(gap_with_extra_vars)
gap %>% select(continent, year) %>% tail()
head(gap)
# Load the "tidyr" package (necessary every new R session)
library(tidyr)
gap_wide <- read.csv("../data/gapminder_wide.csv", stringsAsFactors = FALSE)
gap_wide <- read.csv("F:\\r-bootcamp-fall-2019-master\\r-bootcamp-fall-2019-master\\modulesgapminder_wide.csv", stringsAsFactors = FALSE)
head(gap_wide)
gap_wide <- read.csv("F:\\r-bootcamp-fall-2019-master\\r-bootcamp-fall-2019-master\\modulesgapminder_wide.csv", stringsAsFactors = FALSE)
getwd()
setwd("C:\Users\Aditya Peshin\Desktop\Studies\UC Berkeley\Fall 2019\242 - Applications in Data Analysis\course_files_export\Lab Discussions\Lab 2")
setwd("\Users\Aditya Peshin\Desktop\Studies\UC Berkeley\Fall 2019\242 - Applications in Data Analysis\course_files_export\Lab Discussions\Lab 2")
setwd("C:\\Users\\Aditya Peshin\\Desktop\\Studies\\UC Berkeley\\Fall 2019\\242 - Applications in Data Analysis\\course_files_export\\Lab Discussions\\Lab 2")
getwd()
install.packages(c("car", "GGally"))
library(dplyr)
library(ggplot2)
library(GGally)
# install.packages("car")
library(car) # for VIF
# Load data:
wine <- read.csv("wine_agg.csv")
str(wine)
head(wine)
# Plot scatter matrix
# density plots on the diagonal and correlation printed in the upper triangle
ggscatmat(wine, columns = 2:9, alpha = 0.8)
# Plot scatter matrix
# density plots on the diagonal and correlation printed in the upper triangle
ggscatmat(wine, columns = 2:9, alpha = 0.1)
# Plot scatter matrix
# density plots on the diagonal and correlation printed in the upper triangle
ggscatmat(wine, columns = 2:9, alpha = 0.8)
wine.train <- filter(wine, Year <= 1985)
head(wine.train)
tail(wine.train)
wine.test <- filter(wine, Year > 1985)
nrow(filter(wine, Age>40))
# train the model
#lm(y~x1+x2+...,data)
mod1 <- lm(LogAuctionIndex ~ WinterRain + HarvestRain + GrowTemp + HarvestTemp + Age + FrancePop + USAlcConsump,
data = wine.train)
summary(mod1)
summary(mod1)
winePredictions <- predict(mod1, newdata=wine.test)
# this builds a vector of predicted values on the test set
SSE = sum((wine.test$LogAuctionIndex - winePredictions)^2)
SST = sum((wine.test$LogAuctionIndex - mean(wine.train$LogAuctionIndex))^2)
OSR2 = 1 - SSE/SST
# Confidence interval plot
ggcoef(
mod1,
vline_color = "red",
vline_linetype =  "solid",
errorbar_color = "blue",
errorbar_height = .25,
exclude_intercept = TRUE
)
install.packages("broom")
# Confidence interval plot
ggcoef(
mod1,
vline_color = "red",
vline_linetype =  "solid",
errorbar_color = "blue",
errorbar_height = .25,
exclude_intercept = TRUE
)
################################
vif(mod1)
# A better model...
# Remove FrancePop
mod2 <- lm(LogAuctionIndex ~ WinterRain + HarvestRain + GrowTemp + HarvestTemp + Age + USAlcConsump,
data = wine.train)
summary(mod2)
vif(mod2)
# Remove USAlcConsump
mod3 <- lm(LogAuctionIndex ~ WinterRain + HarvestRain + GrowTemp + HarvestTemp + Age,
data = wine.train)
summary(mod3)
vif(mod3)
# Remove HarvestTemp
mod4 <- lm(LogAuctionIndex ~ WinterRain + HarvestRain + GrowTemp + Age,
data = wine.train)
summary(mod4)
vif(mod4)
mod5 <- lm(LogAuctionIndex ~ WinterRain + HarvestRain+ GrowTemp*Age,
data = wine.train)
summary(mod5)
getwd()
setwd("C:\\Users\\Aditya Peshin\\Desktop\\Studies\\UC Berkeley\\Fall 2019\\242 - Applications in Data Analysis\\course_files_export\\Assignments\\hw 1")
getwd(())
getwd()
#loading in the data
wrangler_orig <- read.csv("Wrangler242-Fall2019.csv")
View(wrangler_orig)
ggscatmat(wrangler_orig, columns = 2:8, alpha = 0.8)
#loading in the data
wrangler_orig <- read.csv("Wrangler242-Fall2019.csv")
View(wrangler_orig)
ggscatmat(wrangler_orig, columns = 2:8, alpha = 0.8)
training_set <- filter(wrangler_orig, Year<= 2015)
testing_set <- filter(wrangler_orig, Year >= 2016)
mod1 <- lm(WranglerSales ~ Unemployment + WranglerQueries + CPI.Energy + CPI.All)
mod1 <- lm(WranglerSales ~ Unemployment + WranglerQueries + CPI.Energy + CPI.All, data = training_set)
model1 <- lm(WranglerSales ~ Unemployment + WranglerQueries + CPI.Energy + CPI.All, data = training_set)
summary(model1)
vif(model1)
model2 <- lm(WranglerSales ~ Unemployment + WranglerQueries + CPI.Energy, data = training_set)
summary(model2)
vif(model2)
model3 <- lm(WranglerSales ~ WranglerQueries + CPI.Energy, data = training_set)
summary(model3)
vif(model3)
model4 <- lm(WranglerSales ~ WranglerQueries, data = training_set)
summary(model4)
vif(model4)
SalesPrediction1 <- predict(model1, newdata=testing_set)
SSE1 = sum((testing_set$WranglerSales - SalesPrediction1)^2)
SST1 = sum((testing_set$WranglerSales - mean(training_set$WranglerSales))^2)
OSRsquared1 = 1 - SSE1/SST1
SalesPrediction2 <- predict(model2, newdata=testing_set)
SSE2 = sum((testing_set$WranglerSales - SalesPrediction2)^2)
SST2 = sum((testing_set$WranglerSales - mean(training_set$WranglerSales))^2)
OSRsquared2 = 1 - SSE2/SST2
SSE3 = sum((testing_set$WranglerSales - SalesPrediction3)^2)
SalesPrediction3 <- predict(model3, newdata=testing_set)
SSE3 = sum((testing_set$WranglerSales - SalesPrediction3)^2)
SST3 = sum((testing_set$WranglerSales - mean(training_set$WranglerSales))^2)
OSRsquared3 = 1 - SSE3/SST3
SalesPrediction4 <- predict(model4, newdata=testing_set)
SSE4 = sum((testing_set$WranglerSales - SalesPrediction4)^2)
SST4 = sum((testing_set$WranglerSales - mean(training_set$WranglerSales))^2)
OSRsquared4 = 1 - SSE4/SST4
# Question 3 Part 2 (Considering Seasonality)
model5 <- lm(WranglerSales ~ Unemployment + WranglerQueries + CPI.Energy + CPI.All + MonthNumeric, data = training_set)
summary(model1)
vif(model1)
summary(model5)
vif(model5)
summary(model4)
summary(model1)
vif(model1)
model2 <- lm(WranglerSales ~ Unemployment + WranglerQueries + CPI.Energy, data = training_set)
summary(model2)
vif(model2)
model3 <- lm(WranglerSales ~ WranglerQueries + CPI.Energy, data = training_set)
summary(model3)
vif(model3)
model4 <- lm(WranglerSales ~ WranglerQueries, data = training_set)
summary(model4)
vif(model4)
#loading in the data
wrangler_orig <- read.csv("Wrangler242-Fall2019_gasPrice.csv")
#loading in the data
wrangler_orig <- read.csv("Wrangler242-Fall2019_gasPrice.csv")
View(wrangler_orig)
#Search for any corelation in the given data
ggscatmat(wrangler_orig, columns = 2:8, alpha = 0.8)
#Splitting into training and testing set
training_set <- filter(wrangler_orig, Year<= 2015)
testing_set <- filter(wrangler_orig, Year >= 2016)
model1 <- lm(WranglerSales ~ Unemployment + WranglerQueries + CPI.Energy + CPI.All, data = training_set)
summary(model1)
vif(model1)
model2 <- lm(WranglerSales ~ Unemployment + WranglerQueries + CPI.Energy, data = training_set)
summary(model2)
vif(model2)
model3 <- lm(WranglerSales ~ WranglerQueries + CPI.Energy, data = training_set)
summary(model3)
vif(model3)
model4 <- lm(WranglerSales ~ WranglerQueries, data = training_set)
summary(model4)
vif(model4)
#testing model 1 for its OSR Squared
SalesPrediction1 <- predict(model1, newdata=testing_set)
SSE1 = sum((testing_set$WranglerSales - SalesPrediction1)^2)
SST1 = sum((testing_set$WranglerSales - mean(training_set$WranglerSales))^2)
OSRsquared1 = 1 - SSE1/SST1
#testing model 2 for its OSR Squared
SalesPrediction2 <- predict(model2, newdata=testing_set)
SSE2 = sum((testing_set$WranglerSales - SalesPrediction2)^2)
SST2 = sum((testing_set$WranglerSales - mean(training_set$WranglerSales))^2)
OSRsquared2 = 1 - SSE2/SST2
#testing model 3 for its OSR Squared
SalesPrediction3 <- predict(model3, newdata=testing_set)
SSE3 = sum((testing_set$WranglerSales - SalesPrediction3)^2)
SST3 = sum((testing_set$WranglerSales - mean(training_set$WranglerSales))^2)
OSRsquared3 = 1 - SSE3/SST3
#testing model 4 for its OSR Squared
SalesPrediction4 <- predict(model4, newdata=testing_set)
SSE4 = sum((testing_set$WranglerSales - SalesPrediction4)^2)
SST4 = sum((testing_set$WranglerSales - mean(training_set$WranglerSales))^2)
OSRsquared4 = 1 - SSE4/SST4
# Question 3 Part 2 (Considering Seasonality)
model5 <- lm(WranglerSales ~ Unemployment + WranglerQueries + CPI.Energy + CPI.All + MonthNumeric, data = training_set)
summary(model5)
vif(model5)
# Question 3 Part 2 (Considering Seasonality)
model5 <- lm(WranglerSales ~ Unemployment + WranglerQueries + CPI.Energy + CPI.All + MonthFactor, data = training_set)
summary(model5)
vif(model5)
summary(model5)
vif(model5)
# Question 3 Part c (Building a better model)
model6 <- lm(WranglerSales ~ WranglerQueries + MonthFactor, data = training_set)
summary(model6)
vif(model6)
#testing model 6 for its OSR Squared
SalesPrediction6 <- predict(model6, newdata=testing_set)
SSE6 = sum((testing_set$WranglerSales - SalesPrediction6)^2)
SST6 = sum((testing_set$WranglerSales - mean(training_set$WranglerSales))^2)
OSRsquared6 = 1 - SSE6/SST6
# Question 3 part d (adding oil price data)
model7 <- lm(WranglerSales ~ WranglerQueries + MonthFactor + GasolinePrice, data = training_set)
summary(model7)
vif(model7)
#testing model 7 for its OSR Squared
SalesPrediction7 <- predict(model7, newdata=testing_set)
SSE7 = sum((testing_set$WranglerSales - SalesPrediction7)^2)
SST7 = sum((testing_set$WranglerSales - mean(training_set$WranglerSales))^2)
OSRsquared7 = 1 - SSE7/SST7
