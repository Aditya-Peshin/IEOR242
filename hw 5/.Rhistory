load("D:/Studies/Berkeley/Semester 1/242/Hw 5/variables.RData")
library(softImpute)
library(randomForest)
library(ranger)
library(dplyr)
library(tidyverse)
library(reshape2)
setwd("D:\\Studies\\Berkeley\\Semester 1\\242\\Hw 5")
View(val1)
# Part (C.ii)
# Trying different number of archetypes, i.e. rank = 1,2,...,20
CF.mae.vals = rep(NA, 20)
for (rnk in seq_len(20)) {
print(str_c("Trying rank.max = ", rnk))
CFmod <- softImpute(mat.train.centered, rank.max = rnk, lambda = 0, maxit = 1000)
preds <- impute(CFmod, val1$userID, val1$songID) %>% pmin(5) %>% pmax(1)
preds2 <- preds - aplha[val1$userID] - beta[val1$songID]
CF.mae.vals[rnk] <- mean(abs(preds2 - val1$rating))
}
for (rnk in seq_len(20)) {
print(str_c("Trying rank.max = ", rnk))
CFmod <- softImpute(mat.train.centered, rank.max = rnk, lambda = 0, maxit = 1000)
preds <- impute(CFmod, val1$userID, val1$songID) %>% pmin(5) %>% pmax(1)
preds2 <- preds - alpha[val1$userID] - beta[val1$songID]
CF.mae.vals[rnk] <- mean(abs(preds2 - val1$rating))
}
CF.mae.val.df <- data.frame(rnk = seq_len(20), mae = CF.mae.vals)
ggplot(CF.mae.val.df, aes(x = rnk, y = mae)) + geom_point(size = 3) +
ylab("Validation MAE") + xlab("Number of Archetypal Users") +
theme_bw() + theme(axis.title=element_text(size=18), axis.text=element_text(size=18))
# Part (C.ii)
# Trying different number of archetypes, i.e. rank = 1,2,...,20
CF.mae.vals = rep(NA, 20)
for (rnk in seq_len(20)) {
print(str_c("Trying rank.max = ", rnk))
CFmod <- softImpute(mat.train.centered, rank.max = rnk, lambda = 0, maxit = 1000)
preds <- impute(CFmod, val1$userID, val1$songID) %>% pmin(5) %>% pmax(1)
preds2 <- preds #- alpha[val1$userID] - beta[val1$songID]
CF.mae.vals[rnk] <- mean(abs(preds2 - val1$rating))
}
CF.mae.val.df <- data.frame(rnk = seq_len(20), mae = CF.mae.vals)
ggplot(CF.mae.val.df, aes(x = rnk, y = mae)) + geom_point(size = 3) +
ylab("Validation MAE") + xlab("Number of Archetypal Users") +
theme_bw() + theme(axis.title=element_text(size=18), axis.text=element_text(size=18))
# Building a model using only genre of the song
# Trying linear regression
train2 <- train
train2$genre <- songs$genre[train2$songID]
train2$year <- songs$year[train2$songID]
modLM <- lm(rating ~ genre, data = train2)
summary(modLM)
test2$genre <- songs$genre[test2$songID]
test2$year <- songs$year[test2$songID]
modLMpreds <- predict(modLM, newdata=test2)
OSR2(modLMpreds, train2$rating, test2$rating)
val2a <- val2
val2a$genre <- songs$genre[val2a$songID]
val2a$year <- songs$year[val2a$songID]
modLM <- lm(rating ~ genre, data = train2)
summary(modLM)
modLMpreds <- predict(modLM, newdata=val2a)
OSR2(modLMpreds, train2$rating, val2a$rating)
modRF <- randomForest(rating ~ genre, data = train2)
gc()
modRF <- randomForest(rating ~ genre, data = train2)
gc()
memory.limit()
memory.size()
memory.limit()
save.image("D:/Studies/Berkeley/Semester 1/242/Hw 5/variables_11.28.RData")
setwd("D:\\Studies\\Berkeley\\Semester 1\\242\\Hw 5")
library(softImpute)
library(randomForest)
library(ranger)
library(dplyr)
library(tidyverse)
library(reshape2)
OSR2 <- function(predictions, train, test) {
SSE <- sum((test - predictions)^2)
SST <- sum((test - mean(train))^2)
r2 <- 1 - SSE/SST
return(r2)
}
musicratings <- read.csv("D:\\Studies\\Berkeley\\Semester 1\\242\\Hw 5\\MusicRatings.csv")
songs <- read.csv("D:\\Studies\\Berkeley\\Semester 1\\242\\Hw 5\\Songs.csv")
users <- read.csv("D:\\Studies\\Berkeley\\Semester 1\\242\\Hw 5\\Users.csv")
set.seed(345)
train.ids <- sample(nrow(musicratings), 0.92*nrow(musicratings))
train <- musicratings[train.ids,]
test <- musicratings[-train.ids,]
val1.ids <- sample(nrow(train), (4/92)*nrow(train))
val1 <- train[val1.ids,]
train <- train[-val1.ids,]
val2.ids <- sample(nrow(train), (4/88)*nrow(train))
val2 <- train[val2.ids,]
train <- train[-val2.ids,]
mat.train <- Incomplete(train$userID, train$songID, train$rating)
set.seed(345)
mat.train.centered <- biScale(mat.train, maxit = 1000, row.scale = FALSE, col.scale = FALSE)
alpha <- attr(mat.train.centered, "biScale:row")$center
beta <- attr(mat.train.centered, "biScale:column")$center
train2 <- train
train2$genre <- songs$genre[train2$songID]
train2$year <- songs$year[train2$songID]
test2$genre <- songs$genre[test2$songID]
test2$year <- songs$year[test2$songID]
val2a <- val2
val2a$genre <- songs$genre[val2a$songID]
val2a$year <- songs$year[val2a$songID]
test2 <- test
test2$genre <- songs$genre[test2$songID]
test2$year <- songs$year[test2$songID]
modRF <- randomForest(rating ~ genre, data = train2)
modLM <- lm(rating ~ genre, data = train2)
summary(modLM)
modLMpreds <- predict(modLM, newdata=val2a)
OSR2(modLMpreds, train2$rating, val2a$rating)
library(glmnet)
library(broom)
lambdas <- 10^seq(3,-2, by = -0.2)
modRidge <- glmnet(train2$rating ~ train2$genre,alpha = 0, lambda = lambdas)
modRidge <- glmnet(y = train2$rating, x = train2$genre,alpha = 0, lambda = lambdas)
modRidge <- glmnet(x = train2$genre , y = train2$rating ,alpha = 0, lambda = lambdas)
str(train2$genre)
modRidge <- glmnet(genre, rating, data = train2, alpha = 0, lambda = lambdas)
# Building a model using only the year the song was released
modLM2 <- lm(rating ~ genre, data = train2)
summary(modLM2)
modLM2preds <- predict(modLM2, newdata=val2a)
OSR2(modLM2preds, train2$rating, val2a$rating)
# Building a model using only the year the song was released
modLM2 <- lm(rating ~ year, data = train2)
summary(modLM2)
modLM2preds <- predict(modLM2, newdata=val2a)
OSR2(modLM2preds, train2$rating, val2a$rating)
# Calculating the MAE
mean(abs(modLMpreds - val2a$rating))/5
# Calculating the RSS
sqrt(mean((modLMpreds - val2a$rating)^2))/5
# Calculating R^2
OSR2(modLMpreds, train2$rating, val2a$rating)
# Calculating the MAE
mean(abs(modLM2preds - val2a$rating))/5
# Calculating the RSS
sqrt(mean((modLM2preds - val2a$rating)^2))/5
# Calculating R^2
OSR2(modLM2preds, train2$rating, val2a$rating)
# Part (C.ii)
# Trying different number of archetypes, i.e. rank = 1,2,...,20
CF.mae.vals = rep(NA, 20)
for (rnk in seq_len(20)) {
print(str_c("Trying rank.max = ", rnk))
CFmod <- softImpute(mat.train.centered, rank.max = rnk, lambda = 0, maxit = 1000)
preds <- impute(CFmod, val1$userID, val1$songID) %>% pmin(5) %>% pmax(1)
CF.mae.vals[rnk] <- mean(abs(preds - val1$rating))
}
CF.mae.val.df <- data.frame(rnk = seq_len(20), mae = CF.mae.vals)
ggplot(CF.mae.val.df, aes(x = rnk, y = mae)) + geom_point(size = 3) +
ylab("Validation MAE") + xlab("Number of Archetypal Users") +
theme_bw() + theme(axis.title=element_text(size=18), axis.text=element_text(size=18))
set.seed(345)
finalCFmod <- softImpute(mat.train.centered, rank.max = 5, lambda = 0, maxit = 1000)
finalCFpreds <- impute(finalCFmod, test$userID, test$songID) %>% pmin(5) %>% pmax(1)
# Calculating the MAE
mean(abs(finalCFpreds - test$rating))/5
# Calculating the RSS
sqrt(mean((finalCFpreds - test$rating)^2))/5
# Calculating R^2
OSR2(finalCFpreds, train$rating, test$rating)
modLMpreds <- predict(modLM, newdata=test2)
# Calculating the MAE
mean(abs(modLMpreds - test2$rating))/5
# Calculating the RSS
sqrt(mean((modLMpreds - test2$rating)^2))/5
# Calculating R^2
OSR2(modLMpreds, train2$rating, test2$rating)
modLM2preds <- predict(modLM2, newdata=test2)
# Calculating the MAE
mean(abs(modLM2preds - test2$rating))/5
# Calculating the RSS
sqrt(mean((modLM2preds - test2$rating)^2))/5
# Calculating R^2
OSR2(modLM2preds, train2$rating, test2$rating)
val2predsCF <- impute(finalCFmod, val2a$userID, val2a$songID) %>% pmin(5) %>% pmax(1)
val2predsLMG <- predict(modLM, newdata=val2a)
val2predsLMY <- predict(modLM2, newdata=val2a)
val.blending_df = data.frame(rating = val2a$rating, cf_preds = val2predsCF, lm1_preds = val2predsLMG , lm2_preds =val2predsLMY)
mod_blend = lm(rating ~ . -1, data = val.blending_df)
summary(mod_blend)
test2CF  <- impute(finalCFmod, test2$userID, test2$songID) %>% pmin(5) %>% pmax(1)
test2predsLMG <- predict(modLM, newdata=test2)
test2predsLMY <- predict(modLM2, newdata=test2)
test.blending_df = data.frame(rating = test2$rating, cf_preds = test2predsCF, lm1_preds = test2predsLMG , lm2_preds =test2predsLMY)
test.blending_df = data.frame(rating = test2$rating, cf_preds = test2CF, lm1_preds = test2predsLMG , lm2_preds =test2predsLMY)
test.preds.blend <- predict(mod_blend, newdata = test.blending_df)
mean(abs(test.preds.blend - test$rating))/4
sqrt(mean((test.preds.blend - test$rating)^2))/4
OSR2(test.preds.blend, train$rating, test$rating)
testa <- test
testa$alpha <- alpha[testa$userID]
testa$beta <- beta[testa$songID]
testa$preds <- testa$alpha + testa$beta
OSR2(testa$preds, train$rating, testa$rating)
# Building a model using only the year the song was released
modLM2 <- randomForest(rating ~ year, data = train2)
?randomForest.default
# Building a model using only the year the song was released
modLM2 <- randomForest(rating ~ year, data = train2, mtry = 1, ntree = 500)
library(rpart)
?rpart
library(caret)
# Building a model using only the year the song was released
set.seed(456)
cpVals = data.frame(cp = seq(0, .04, by=.002))
modCART <- train(rating ~ year,
data = train2,
method = "rpart",
tuneGrid = cpVals,
trControl = trainControl(method = "cv", number=10))
modCART$results
modCARTpreds <- predict(modCART, newdata=test2)
# Calculating the MAE
mean(abs(modCARTpreds - test2$rating))/5
# Calculating the RSS
sqrt(mean((modCARTpreds - test2$rating)^2))/5
# Calculating R^2
OSR2(modCARTpreds, train2$rating, test2$rating)
val2predsLMY <- predict(modCART, newdata=val2a)
mod_blend = lm(rating ~ . -1, data = val.blending_df)
summary(mod_blend)
test2predsLMY <- predict(modCART, newdata=test2)
test.blending_df = data.frame(rating = test2$rating, cf_preds = test2CF, lm1_preds = test2predsLMG , lm2_preds =test2predsLMY)
test.preds.blend <- predict(mod_blend, newdata = test.blending_df)
mean(abs(test.preds.blend - test$rating))/4
sqrt(mean((test.preds.blend - test$rating)^2))/4
OSR2(test.preds.blend, train$rating, test$rating)
library(boot)
mean_squared_error <- function(data, index) {
responses <- data$response[index]
predictions <- data$prediction[index]
MSE <- mean((responses - predictions)^2)
return(MSE)
}
mean_absolute_error <- function(data, index) {
responses <- data$response[index]
predictions <- data$prediction[index]
MAE <- mean(abs(responses - predictions))
return(MAE)
}
OS_R_squared <- function(data, index) {
responses <- data$response[index]
predictions <- data$prediction[index]
baseline <- data$baseline[index]
SSE <- sum((responses - predictions)^2)
SST <- sum((responses - baseline)^2)
r2 <- 1 - SSE/SST
return(r2)
}
all_metrics <- function(data, index) {
mse <- mean_squared_error(data, index)
mae <- mean_absolute_error(data, index)
OSR2 <- OS_R_squared(data, index)
return(c(mse, mae, OSR2))
}
LR_test_set = data.frame(response = test2$rating, prediction = test.preds.blend , baseline = mean(train2$rating))
# sanity check
all_metrics(LR_test_set, 1:1818)
# sanity check
all_metrics(LR_test_set, 1:LR_test_set.shape[0])
length()
?length()
?nrow
# sanity check
all_metrics(LR_test_set, 1:nrow(LR_test_set))
mean((pred.mod.lm - test.ctr$CTR)^2)
mean((test.preds.blend - test.ctr$CTR)^2)
# sanity check
all_metrics(LR_test_set, 1:nrow(LR_test_set))
mean((test.preds.blend - test2$rating)^2)
mean(abs(test.preds.blend - test2$rating))
OSR2(test.preds.blend, test2$rating, train2$rating)
OSR2(test.preds.blend, test2$rating, mean(train2$rating))
OSR2(test.preds.blend, test2$rating)
OSR2(test.preds.blend, train2$rating, test2$rating)
# sanity check
all_metrics(LR_test_set, 1:nrow(LR_test_set))
# do bootstrap
set.seed(456)
LR_boot <- boot(LR_test_set, all_metrics, R = 1000)
LR_boot
# get confidence intervals
boot.ci(LR_boot, index = 1, type = "basic")
boot.ci(LR_boot, index = 2, type = "basic")
boot.ci(LR_boot, index = 3, type = "basic")
blend_test_set = data.frame(response = test2$rating, prediction = test.preds.blend , baseline = mean(train2$rating))
# sanity check
all_metrics(blend_test_set, 1:nrow(LR_test_set))
mean((test.preds.blend - test2$rating)^2)
mean(abs(test.preds.blend - test2$rating))
OSR2(test.preds.blend, train2$rating, test2$rating)
# do bootstrap
set.seed(456)
blend_boot <- boot(blend_test_set, all_metrics, R = 1000)
blend_boot
# get confidence intervals
boot.ci(blend_boot, index = 1, type = "basic")
boot.ci(blend_boot, index = 2, type = "basic")
boot.ci(blend_boot, index = 3, type = "basic")
# CART(year) bootstrap model metrics
CART_test_set = data.frame(response = test2$rating, prediction = modCARTpreds, baseline = mean(train2$rating))
# sanity check
all_metrics(CART_test_set, 1:nrow(LR_test_set))
mean((test.preds.blend - test2$rating)^2)
mean(abs(test.preds.blend - test2$rating))
OSR2(test.preds.blend, train2$rating, test2$rating)
# sanity check
all_metrics(CART_test_set, 1:nrow(LR_test_set))
mean((modCARTpreds - test2$rating)^2)
mean(abs(modCARTpreds - test2$rating))
OSR2(modCARTpreds, train2$rating, test2$rating)
# do bootstrap
set.seed(456)
CART_boot <- boot(CART_test_set, all_metrics, R = 1000)
# get confidence intervals
boot.ci(CART_boot, index = 1, type = "basic")
boot.ci(CART_boot, index = 2, type = "basic")
boot.ci(CART_boot, index = 3, type = "basic")
# LR(genre) bootstrap model metrics
LRG_test_set = data.frame(response = test2$rating, prediction = modLMpreds, baseline = mean(train2$rating))
# sanity check
all_metrics(LRG_test_set, 1:nrow(LRG_test_set))
mean((modLMpreds - test2$rating)^2)
mean(abs(modLMpreds - test2$rating))
OSR2(modLMpreds, train2$rating, test2$rating)
# do bootstrap
set.seed(456)
LRG_boot <- boot(LRG_test_set, all_metrics, R = 1000)
# get confidence intervals
boot.ci(LRG_boot, index = 1, type = "basic")
boot.ci(LRG_boot, index = 2, type = "basic")
boot.ci(LRG_boot, index = 3, type = "basic")
# CF bootstrap model metrics
CF_test_set = data.frame(response = test2$rating, prediction = finalCFpreds, baseline = mean(train2$rating))
# sanity check
all_metrics(CF_test_set, 1:nrow(LRG_test_set))
mean((finalCFpreds - test2$rating)^2)
mean(abs(finalCFpreds - test2$rating))
OSR2(finalCFpreds, train2$rating, test2$rating)
# do bootstrap
set.seed(456)
CF_boot <- boot(CF_test_set, all_metrics, R = 1000)
# get confidence intervals
boot.ci(CF_boot, index = 1, type = "basic")
boot.ci(CF_boot, index = 2, type = "basic")
boot.ci(CF_boot, index = 3, type = "basic")
CF_boot
save.image("D:/Studies/Berkeley/Semester 1/242/Hw 5/variables_11.28 working.RData")
# Part (C.ii)
# Trying different number of archetypes, i.e. rank = 1,2,...,20
CF.mae.vals = rep(NA, 20)
for (rnk in seq_len(20)) {
print(str_c("Trying rank.max = ", rnk))
CFmod <- softImpute(mat.train.centered, rank.max = rnk, lambda = 0, maxit = 1000)
preds <- impute(CFmod, val1$userID, val1$songID, unscale = TRUE) %>% pmin(3.433) %>% pmax(1)
# here the impute function has a parameter called "unscale" which is set to true
# This parameter, when set to true, reverses the centering and scaling on the predictions.
CF.mae.vals[rnk] <- mean(abs(preds - val1$rating))
}
CF.mae.val.df <- data.frame(rnk = seq_len(20), mae = CF.mae.vals)
ggplot(CF.mae.val.df, aes(x = rnk, y = mae)) + geom_point(size = 3) +
ylab("Validation MAE") + xlab("Number of Archetypal Users") +
theme_bw() + theme(axis.title=element_text(size=18), axis.text=element_text(size=18))
# Part (C.iii)
set.seed(345)
finalCFmod <- softImpute(mat.train.centered, rank.max = 5, lambda = 0, maxit = 1000)
finalCFpreds <- impute(finalCFmod, test$userID, test$songID) %>% pmin(3.433) %>% pmax(1)
# Calculating the MAE
mean(abs(finalCFpreds - test$rating))/5
# Calculating the RSS
sqrt(mean((finalCFpreds - test$rating)^2))/5
# Calculating R^2
OSR2(finalCFpreds, train$rating, test$rating)
val2predsCF <- impute(finalCFmod, val2a$userID, val2a$songID) %>% pmin(3.433) %>% pmax(1)
val.blending_df = data.frame(rating = val2a$rating, cf_preds = val2predsCF, lm1_preds = val2predsLMG , lm2_preds =val2predsLMY)
mod_blend = lm(rating ~ . -1, data = val.blending_df)
test2CF  <- impute(finalCFmod, test2$userID, test2$songID) %>% pmin(3.433) %>% pmax(1)
test.blending_df = data.frame(rating = test2$rating, cf_preds = test2CF, lm1_preds = test2predsLMG , lm2_preds =test2predsLMY)
test.preds.blend <- predict(mod_blend, newdata = test.blending_df)
mean(abs(test.preds.blend - test$rating))/4
sqrt(mean((test.preds.blend - test$rating)^2))/4
OSR2(test.preds.blend, train$rating, test$rating)
# Bootstrapping to test confidence intervals of new model
# Bootstrapping Blended model metrics
blend_test_set = data.frame(response = test2$rating, prediction = test.preds.blend , baseline = mean(train2$rating))
# do bootstrap
set.seed(456)
blend_boot <- boot(blend_test_set, all_metrics, R = 1000)
# get confidence intervals
boot.ci(blend_boot, index = 1, type = "basic")
boot.ci(blend_boot, index = 2, type = "basic")
boot.ci(blend_boot, index = 3, type = "basic")
# CF bootstrap model metrics
CF_test_set = data.frame(response = test2$rating, prediction = finalCFpreds, baseline = mean(train2$rating))
# do bootstrap
set.seed(456)
CF_boot <- boot(CF_test_set, all_metrics, R = 1000)
# get confidence intervals
boot.ci(CF_boot, index = 1, type = "basic")
boot.ci(CF_boot, index = 2, type = "basic")
boot.ci(CF_boot, index = 3, type = "basic")
23153*807
807*2421
View(mat.train.centered)
